{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x_train_df = pd.read_csv('data/data_reviews/x_train.csv')\n",
    "y_train_df = pd.read_csv('data/data_reviews/y_train.csv')\n",
    "\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "for text in tr_text_list:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the data\n",
    "\n",
    "Natural Language ToolKit (nltk) is used to preprocess the data.\n",
    "\n",
    "1. Turn all sentences to lowercase\n",
    "2. Delete apostrophes\n",
    "3. Delete punctuations\n",
    "4. Remove stop words\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# word count \n",
    "cv = CountVectorizer()\n",
    "word_count_vector = cv.fit_transform(tr_text_list)\n",
    "print(word_count_vector.shape)\n",
    "\n",
    "# apostrophes\n",
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"\n",
    "}\n",
    "\n",
    "# Preprocess Data\n",
    "def preprocess(review_arr):\n",
    "  processed = []\n",
    "\n",
    "  # lowercase\n",
    "  for text in review_arr:\n",
    "    processed.append(text.lower())\n",
    "\n",
    "  # convert apostrophes to standard lexicon \n",
    "  negation = []\n",
    "  for text in processed:\n",
    "    words = text.split()\n",
    "    reformed = [appos[word] if word in appos else word for word in words]\n",
    "    reformed = \" \".join(reformed)\n",
    "    negation.append(reformed)\n",
    "\n",
    "  # tokenize data by converting text to tokens\n",
    "  tokenized = []\n",
    "  for text in negation:\n",
    "    tokenized.append(word_tokenize(text))\n",
    "\n",
    "  # remove stopwords\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  for text in tokenized:\n",
    "    text = [i for i in text if not i in stop_words]\n",
    "  \n",
    "  # remove stand-alone punctuation\n",
    "  stripped = []\n",
    "  for text in tokenized:\n",
    "    words = [word for word in text if word.isalpha()]\n",
    "    stripped.append(words)\n",
    "\n",
    "  # lemmatization\n",
    "  lemmas = []\n",
    "  porter = PorterStemmer()\n",
    "  for text in stripped:\n",
    "    lemmafied = []\n",
    "    for t in text:\n",
    "      lemmafied.append(porter.stem(t))\n",
    "    lemmas.append(lemmafied)\n",
    "\n",
    "  return lemmas\n",
    "\n",
    "\n",
    "p = preprocess(tr_text_list)\n",
    "for i in range(20):\n",
    "    print(p[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize preprocessed data to feature vectors using Bag of Words Model and TF-IDF (TfidVectorizer and TfidTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# feature transform training set\n",
    "x_train_df = pd.read_csv('data/data_reviews/x_train.csv')\n",
    "y_train_df = pd.read_csv('data/data_reviews/y_train.csv')\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "x_tr_pre = preprocess(tr_text_list)\n",
    "x_tr = []\n",
    "for text in x_tr_pre:\n",
    "  sentence = \" \".join(text)\n",
    "  x_tr.append(sentence)\n",
    "x_train = vectorizer.fit_transform(x_tr)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# feature transform testing set\n",
    "x_test_df = pd.read_csv('data/data_reviews/x_test.csv')\n",
    "te_text_list = x_test_df['text'].values.tolist()\n",
    "x_te_pre = preprocess(te_text_list)\n",
    "x_te = [] \n",
    "for text in x_te_pre:\n",
    "  sentence = \" \".join(text)\n",
    "  x_te.append(sentence)\n",
    "x_test = vectorizer.transform(x_te)\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Grid Search for Hyperparameters\n",
    "penalty = ['none','l2']\n",
    "C = np.logspace(0, 6, 12)\n",
    "hyperparams = dict(C=C, penalty=penalty)\n",
    "clf = GridSearchCV(logreg, hyperparams, cv=5, verbose=0)\n",
    "clf.fit(x_train, y_train_df)\n",
    "print(\"BEST SCORE: \")\n",
    "print(clf.best_score_)\n",
    "print(\"STANDARD DEVIATIONS\")\n",
    "print(clf.cv_results_['std_test_score'])\n",
    "print(\"STANDARD DEVIATION FOR BEST SCORE:\")\n",
    "print(clf.cv_results_['std_test_score'][clf.best_index_])\n",
    "\n",
    "pivot = pd.pivot_table(pd.DataFrame(clf.cv_results_), values='mean_test_score', index='param_C', columns='param_penalty')\n",
    "ax = sns.heatmap(pivot)\n",
    "\n",
    "yproba1_test = clf.predict_proba(x_test)[:, 1]\n",
    "np.savetxt('logreg_yproba1_test.txt', yproba1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP) Model with Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "mlp = MLPClassifier(max_iter=120)\n",
    "\n",
    "hyperparams = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (256,)],\n",
    "    'activation': ['identity','logistic', 'relu', 'tanh'],\n",
    "}\n",
    "\n",
    "clf_mlp = GridSearchCV(mlp, hyperparams, cv=5, verbose=0)\n",
    "clf_mlp.fit(x_train, y_train_df.values.ravel()) \n",
    "print(\"BEST SCORE: \")\n",
    "print(clf_mlp.best_score_)\n",
    "print(\"STANDARD DEVIATIONS\")\n",
    "print(clf_mlp.cv_results_['std_test_score'])\n",
    "print(\"STANDARD DEVIATION FOR BEST SCORE:\")\n",
    "print(clf_mlp.cv_results_['std_test_score'][clf_mlp.best_index_])\n",
    "\n",
    "pivot_mlp = pd.pivot_table(pd.DataFrame(clf_mlp.cv_results_), values='mean_test_score', index='param_hidden_layer_sizes', columns='param_activation')\n",
    "\n",
    "yproba1_test = clf_mlp.predict_proba(x_test)[:, 1]\n",
    "np.savetxt('mlp_yproba1_test.txt', yproba1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_mlp = sns.heatmap(pivot_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors with Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "hyperparams = {\n",
    "    'n_neighbors' : [1,3,5,7,9,11,13,17,19,21,23,25,50,100],\n",
    "    'weights' : ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "clf_knn = GridSearchCV(KNeighborsClassifier(), hyperparams, cv=5, verbose=0)\n",
    "clf_knn.fit(x_train, y_train_df.values.ravel()) \n",
    "print(\"BEST SCORE: \")\n",
    "print(clf_knn.best_score_)\n",
    "print(\"STANDARD DEVIATIONS\")\n",
    "print(clf_knn.cv_results_['std_test_score'])\n",
    "print(\"STANDARD DEVIATION FOR BEST SCORE:\")\n",
    "print(clf_knn.cv_results_['std_test_score'][clf_knn.best_index_])\n",
    "\n",
    "pivot_knn = pd.pivot_table(pd.DataFrame(clf_knn.cv_results_), values='mean_test_score', index='param_n_neighbors', columns='param_weights',)\n",
    "ax_knn = sns.heatmap(pivot_knn)\n",
    "\n",
    "yproba1_test = clf_knn.predict_proba(x_test)[:, 1]\n",
    "np.savetxt('knn_yproba1_test.txt', yproba1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Logistic Regression classifier performs best among all. Other classifiers like MLP and KNN perform worse. This is partly because of the feature tuning by trying to normalize the input data into feature vectors using TF-IDF. \n",
    "\n",
    "\n",
    "I believe a large part of this success has to do with the feature tuning performed -- by so rigorously trying to normalize the input data into feature vectors using TF-IDF, it seems to me that the Logistic Regression classifier worked the best as it usually performs the best when attributes unrelated to the output variable, as well as closely related attributes are removed from the input set. This was most perceptible within the steps of removing stop words (unrelated attributes to the feature set), and the TF-IDF vectorization (penalizing closely related attributes). In combination with the regularization performed on the model, it seems that normalizing the data did make the output classes rather separable.\n",
    "\n",
    "Using a KNN model may have not been as suited for this task as the number of output classes was limited to 2. Possible reasons for why the KNN model and MLP model may have not worked as best, or in conjunction with my original hypothesis, may also be as a consequence of underfitting, and a lack of greater parameter tuning (which may have resulted in better performance.)\n",
    "\n",
    "The logistic regression model did best on predicting positive values with data sourced from Amazon, with a true positive rate of .9675, compared to .9475 and .935 for Yelp and IMDb, respectively. The model predicted true negatives est fro Yelp reviews, with a true negative rate of 0.98, compared to .9775 and .9725 for Amazon and IMDb, respectively. The model also had a higher false positive rate for Yelp reviews, at a false positive rate of 0.0525, compared to 0.0325 and 0.065 for Amazon and IMDb reviews. Interestingly, it also had the lowest true negative rates for Yelp reviews, at 0.02, compared to 0.0225 and 0.275 for Amazon and IMDb, respectively. Overall, in terms of total accuracy, the model perofrmed best on Amazon reviews, at an accuracy of 0.9725, compared to 0.96375 and 0.95375 for Yelp and IMDb, respectively. Possible reasons for this may be due to the number of sentimental (positive/negative) words within the feature sets for each type of review -- Amazon customers may have better use of language in line with how the model calculates an output class for sentiment, but conversely, my model may also overfit for Amazon reviews.\n",
    "\n",
    "## Applying Best Classifier to Leaderboard\n",
    "\n",
    "Using the test set on the GradeScope leaderboard, the logistic regression model gave an error rate of 0.16167 and an AUROC of 0.9061, putting me 11th place on the leaderboard at the time of writing (out of 64). This matches up with what the training set performance eluded to, given that both the training set and the test set had the best performance compared to the other classfiers. This may suggest that the testing data may be similar in terms of tf-idf values with the training data, and the Logistic Regression model may have overfit on the training data, leading to an increased performance on the testing data (which is similar, in this case). It could also mean that the other models overfit on training data, which in this scenario, is not as similar in comparison to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
